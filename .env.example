# ===================================
# Embedding Backend Configuration
# ===================================

# Choose embedding backend: "huggingface" (fast), "ollama" (flexible), or "lmstudio" (local server)
EMBEDDING_BACKEND=huggingface

# HuggingFace Model (when EMBEDDING_BACKEND=huggingface)
# Recommended: google/embeddinggemma-300m, BAAI/bge-base-en-v1.5
EMBEDDING_MODEL=google/embeddinggemma-300m

# HuggingFace Token (required for gated models like embeddinggemma)
# Get from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_token_here
# or
HF_TOKEN=your_token_here

# ===================================
# LLM Backend Configuration
# ===================================

# Choose LLM backend: "ollama" (default) or "lmstudio" (local server)
LLM_BACKEND=ollama

# ===================================
# Ollama Configuration
# ===================================

OLLAMA_URL=http://localhost:11434

# Ollama Embedding Model (when EMBEDDING_BACKEND=ollama)
OLLAMA_EMBEDDING_MODEL=embeddinggemma

# Ollama Chat/LLM Model (when LLM_BACKEND=ollama)
OLLAMA_CHAT_MODEL=qwen2.5:3b

# Ollama Reranker Model (when LLM_BACKEND=ollama)
OLLAMA_RERANKER_MODEL=dengcao/Qwen3-Reranker-0.6B:Q8_0

# Ollama Configuration
OLLAMA_REQUEST_TIMEOUT=30
OLLAMA_EMBEDDING_BATCH_SIZE=5
OLLAMA_MAX_TOKENS=1000

# ===================================
# LM Studio Configuration
# ===================================

LMSTUDIO_URL=http://localhost:1234/v1

# LM Studio Embedding Model (when EMBEDDING_BACKEND=lmstudio)
LMSTUDIO_EMBEDDING_MODEL=nomic-embed-text

# LM Studio Chat/LLM Model (when LLM_BACKEND=lmstudio)
LMSTUDIO_CHAT_MODEL=hermes-2-pro-llama-3-8b

# LM Studio Reranker Model (when LLM_BACKEND=lmstudio)
LMSTUDIO_RERANKER_MODEL=hermes-2-pro-llama-3-8b

# LM Studio Configuration
LMSTUDIO_REQUEST_TIMEOUT=100
LMSTUDIO_MAX_TOKENS=1000

# ===================================
# Qdrant Configuration
# ===================================

QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION_NAME=islamic_knowledge

# ===================================
# RAG Configuration
# ===================================

VECTOR_SIZE=768
MAX_SOURCES=10
RERANK_WEIGHT=0.7
SCORE_THRESHOLD=0.7
CHUNK_SIZE_MAX=1500
CHUNK_SIZE_MIN=100

# ===================================
# Application Settings
# ===================================

LOG_LEVEL=INFO
SERVER_HOST=0.0.0.0
SERVER_PORT=8123

# ===================================
# Performance Presets
# ===================================

# Fast Ingestion (HuggingFace + GPU) - RECOMMENDED
# EMBEDDING_BACKEND=huggingface
# EMBEDDING_MODEL=BAAI/bge-small-en-v1.5  # 384 dims, fastest

# Best Quality (HuggingFace + large model)
# EMBEDDING_BACKEND=huggingface
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5  # 1024 dims, best quality

# Google EmbeddingGemma (default, good balance)
# EMBEDDING_BACKEND=huggingface
# EMBEDDING_MODEL=google/embeddinggemma-300m  # 256 dims, fast & good

# Ollama Flexibility
# EMBEDDING_BACKEND=ollama
# OLLAMA_EMBEDDING_MODEL=embeddinggemma
# LLM_BACKEND=ollama

# LM Studio (local server, OpenAI-compatible)
# EMBEDDING_BACKEND=lmstudio
# LMSTUDIO_EMBEDDING_MODEL=nomic-embed-text
# LLM_BACKEND=lmstudio
# LMSTUDIO_CHAT_MODEL=hermes-2-pro-llama-3-8b
